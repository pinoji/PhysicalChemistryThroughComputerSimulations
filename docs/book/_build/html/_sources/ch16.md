# Energy, Entropy and Temperature

## Overview

From this point on, we will reconstruct\
\"Thermodynamics\" from partition function.\
The approach we take is to\
define (or \"generate\") thermodynamic variables or function

::: center
:::

introduce free energies using variables & functions defined above.\
E, A, H, G\
introduce 3 laws of Thermodynamics.

introduce chemical potential (chemistry class) and describe equilibrium
from microscopic point of view.

## Derive $<E>$ or $E_\text{system}$. for canonical ensemble

For microcanonical ensemble, system energy $E_{sys}=E$, For canonical
ensemble, we can obtain the energy of the system $<E>$.

We have found that

$$\mathbb{P}_i = \frac{N_i}{N} = \frac{e^{-E_i\beta}}{\sum\limits_i e^{-E_i\beta}}$$

As mentioned previously, we use $Z$ for canonical the partition
function.

$$Z  = \sum\limits_i e^{-\beta E_i}$$

then,

$$<E> = \sum\limits_i E_i \mathbb{P}_i = \frac{\sum\limits_i E_i e^{-\beta E_i}}{\sum\limits_i e^{-\beta E_i}}$$

Since $\frac{d}{d\beta} e^{-\beta E_i} = -E_i e^{-\beta E_i}$,

$$-\left(\frac{\partial \ln Z}{\partial \beta}\right)= -\frac{1}{Z}\left(\frac{\partial Z}{\partial \beta}\right) = \frac{1}{Z}\sum\limits_i E_i e^{-\beta E_i} = <E>$$

Note that we are dealing with NVT ensemble here. For that reason, Our
partition function is denoted as $Z$. Different ensemble uses different
partition function and letter used for those partition function will be
different.\
$\star$ One way to predict the fluctuation of E\
of the system can be achieved by\
calculating variance of energy.\
$$\sigma^2 = <E^2> -<E>^2$$

[HW]{.underline}\
Calculate $\sigma^2$ in terms of $Z$.\

Go over $<E>$ & somehow get $<E^2>$.

## Gibbs Entropy and the Definition of Temperature

Consider a system in contact with a large heat bath, forming a canonical
ensemble. In this ensemble, the system's energy can fluctuate, but the
temperature is held constant by the bath. We start from the **Gibbs
definition of entropy** for the system:

$$\boxed{
S_\mathrm{system} = - k_B \sum_j\mathbb{P}_j(E_j)\ln \mathbb{P}_j (E_j)
}$$

where $\mathbb{P}_j$ is the probability that the system occupies
microstate $j$ with energy $E_j$. Our goal is to determine the
equilibrium distribution $\mathbb{P}_j$ that maximizes
$S_\mathrm{system}$ under the constraints:

$$\sum_j \mathbb{P}_j = 1, \qquad
\sum_j \mathbb{P}_j E_j = \langle E \rangle.$$

### Maximizing entropy using Lagrange multipliers {#maximizing-entropy-using-lagrange-multipliers .unnumbered}

Introduce Lagrange multipliers $\alpha$ and $\beta$ to enforce the
constraints. We extremize the functional:

$$\Phi = S_\mathrm{system} - k_B \alpha \left( \sum_j \mathbb{P}_j - 1 \right) - k_B \beta \left( \sum_j \mathbb{P}_j E_j - \langle E \rangle \right).$$

Taking derivatives with respect to $\mathbb{P}_j$ and setting them to
zero:

$$\frac{\partial \Phi}{\partial \mathbb{P}_j} = -k_B \ln \mathbb{P}_j - k_B + k_B \alpha + k_B \beta E_j = 0
\quad \Rightarrow \quad
\ln \mathbb{P}_j = \alpha - \beta E_j.$$

Exponentiating and normalizing gives the **canonical (Boltzmann)
distribution from the partition function**:

$$\begin{equation}
\mathbb{P}_j = \frac{e^{-\beta E_j}}{Z}, 
\qquad 
Z = \sum_j e^{-\beta E_j}
\tag{Eq 1}
\end{equation}$$

Here $Z$ is the canonical partition function, and $\beta$ is, so far, a
Lagrange multiplier associated with the constraint on the average
energy.

### Connecting $\beta$ to the bath and defining temperature {#connecting-beta-to-the-bath-and-defining-temperature .unnumbered}

The system is in contact with a macroscopic heat bath. The probability
of finding the system in state $j$ is proportional to the number of bath
microstates available when the bath has energy
$E_\mathrm{bath} = E_\mathrm{tot} - E_j$:

$$\mathbb{P}_j \propto \Omega_B(E_\mathrm{bath}) = \Omega_B(E_\mathrm{tot} - E_j).$$

For clarity, this can be written explicitly as:

$$\mathbb{P}_j = \frac{\Omega_B(E_\mathrm{tot} - E_j)}{\Omega_B(E_\mathrm{tot})}.$$

Expanding $\ln \Omega_B(E_\mathrm{tot} - E_j)$ to first order (valid for
a macroscopic bath):

$$\ln \Omega_B(E_\mathrm{tot} - E_j) 
\approx \ln \Omega_B(E_\mathrm{tot}) - E_j \left. \frac{\partial \ln \Omega_B}{\partial E_B} \right|_{E_\mathrm{tot}}.$$

Exponentiating gives the **canonical distribution from the bath**:

$$\begin{equation}
\mathbb{P}_j = \frac{\Omega_B(E_\mathrm{tot} - E_j)}{\Omega_B(E_\mathrm{tot})} \approx e^{- \beta E_j}, 
\qquad 
\beta = \left. \frac{\partial \ln \Omega_B}{\partial E_B} \right|_{E_\mathrm{tot}}
\tag{Eq 2}
\end{equation}$$

This shows that \*\*Eq1 and Eq2 are identical in form\*\*, confirming
that the Lagrange multiplier $\beta$ is physically the slope of the bath
entropy with respect to energy.

### Definition of Temperature

$$\frac{1}{T} = \frac{\partial S_B}{\partial E_B} = k_B \frac{\partial \ln \Omega_B}{\partial E_B} 
\quad \Rightarrow \quad \beta = \frac{1}{k_B T}.$$ The first equality is
the definition of Temperature. Note that the temperature is properly
defined for the **bath**, not the small system inside it:

Since the total energy of the system and bath is fixed,
$$E_\mathrm{tot} = E_\mathrm{system} + E_B,$$ the combined system is
microcanonical, and its total entropy can also be expressed in the
Boltzmann form(see the following section):

$$S_\mathrm{tot} = k_B \ln \Omega_\mathrm{tot}(E_\mathrm{tot}).$$

Because the system is small compared with the bath, the probability of
finding the system in a microstate of energy $E_j$ is proportional to
the number of bath microstates at energy $E_B = E_\mathrm{tot} - E_j$:

$$\mathbb{P}_j \propto \Omega_B(E_\mathrm{tot} - E_j) \approx e^{-\beta E_j}.$$

Thus, the canonical distribution naturally emerges from the
microcanonical description of the bath.

[HW]{.underline} Using the definition of Gibb's entropy, can you show
that S is really extensive? (hint, think of 2 independent system and
think about its probability as a product. )\

## Gibb's Entropy to Boltzmann Entropy for microcanonical ensemble

Before we do this, Gibb's entropy is correct form of entropy but
Boltzmann Entropy is a special form of Gibb's entropy which only works
in microcanonical ensemble (isolated system where each state has equal
probability)\
Starting out with its definition\
$$S_\text{system}= -k_B\sum\limits_j \mathbb{P}_j (E_j)\ln \mathbb{P}_j (E_j) = -k_B \sum_{j}^{\Omega} \mathbb{P}(E)\ln{\mathbb{P}(E)}  \text{\hspace{30pt} for microcanonical ensemble}$$
Since all states are equally probable, $\mathbb{P}(E)=\frac{1}{\Omega}$\
$$S_\text{system}= -k_B \frac{\Omega}{\Omega} \ln{\frac{1}{\Omega}} =k_B \ln{\Omega}$$

$$\text{\hspace{60pt}} S_\text{system}=k_B \ln{\Omega}  \text{\hspace{30pt} Boltzmann entropy}$$

[HW]{.underline} We have used the fact that S is extensive (size
dependent). Using the definition of Boltzmann entropy(microcanonical
ensemble), can you show that S is really extensive? (hint, think of 2
independent system and think about its degeneracy as a whole.
$\Omega_{total}=\Omega_{1}+\Omega_{2}$ )

## The First Law of Thermodynamics

Now that we have defined temperature $T$ and entropy $S$, we can state
the *first law of thermodynamics*, which expresses the conservation of
energy. Energy cannot be created or destroyed; it can only be
transferred between the system and its surroundings.

There are two modes of energy transfer:

- **Work**, denoted $\delta w$,

- **Heat**, denoted $\delta q$.

Because both $\delta w$ and $\delta q$ depend on the path taken by the
system, they are written with the symbol "$\delta$" rather than "$d$".
The internal energy $U$, however, is a state function.

Thus the first law is written as $$dU = \delta q + \delta w.$$

#### Internal energy as a function of $S$ and $V$.

For a simple compressible system, the internal energy can be expressed
as a function of entropy and volume, $$U = U(S,V).$$ Its total
differential is $$dU 
= \left( \frac{\partial U}{\partial S} \right)_V dS
+ \left( \frac{\partial U}{\partial V} \right)_S dV .$$

The thermodynamic definition of work for such a system is
$$\delta w = -P\, dV,$$ where $P$ is the pressure. Comparing this with
the $dV$ term in $dU$ shows that
$$\left( \frac{\partial U}{\partial V} \right)_S = -P .$$

Similarly, by comparing the $dS$ term in $dU$ with $\delta q$, we
identify $$\left( \frac{\partial U}{\partial S} \right)_V = T,$$ which
is consistent with our definition of temperature obtained from
reversible heat transfer.

Substituting both identifications into the total differential yields the
fundamental thermodynamic equation: $$dU = T\, dS - P\, dV.$$

#### Why $S$ and $V$ are the natural variables of $U$.

If the entropy $S$ and volume $V$ of a system are specified and the
system is allowed to relax to equilibrium, the internal energy $U$ takes
on a unique value. Moreover, the partial derivatives of $U(S,V)$
directly give the thermodynamic quantities $T$ and $P$ in their simplest
form: $$T = \left( \frac{\partial U}{\partial S} \right)_V, 
\qquad
-P = \left( \frac{\partial U}{\partial V} \right)_S.$$ For this reason
we say that *$S$ and $V$ are the natural variables of $U$*.

#### Remark.

The derivative $\left( \frac{\partial U}{\partial V} \right)_S$
represents the pressure at constant entropy. Although processes at
exactly constant entropy are difficult to realize in the laboratory, the
expression is extremely useful theoretically. It serves as a starting
point for constructing other thermodynamic potentials via Legendre
transforms, such as the Helmholtz free energy $A(T,V)$, whose natural
variables are often more convenient for practical calculations.

## Classical Definition of Entropy (Classical 1824)

Sadi Carnot published exactly one scientific book in his lifetime: This
is the foundational text of thermodynamics. But Carnot did not derive
entropy---Rudolf Clausius (1850--1865) defined the entropy based on
Carnot idea (he found $\frac{q}{T}$ is important). The concept of
temperature introduced from the bath in the previous section is
consistent with classical thermodynamics. From the first law of
thermodynamics for a system:

$$dE = \delta q + \delta w,$$

we can define a thermodynamic entropy $S$ using reversible processes.

### Why reversible? {#why-reversible .unnumbered}

Entropy is a **state function**, which means it depends only on the
equilibrium state of the system, not on the path taken. However, the
ratio $\delta q / T$ is *not automatically an exact differential* for
arbitrary processes. Only in a **reversible (quasi-static) process**,
where the system is infinitesimally close to equilibrium at all times,
does

$$dS = \frac{\delta q_\mathrm{rev}}{T}$$

become exact. If the process were irreversible, dissipative effects
(e.g., friction, gradients, turbulence) would cause the heat exchange to
differ from the equilibrium energy change, so $\delta q / T$ would not
match the change in entropy.

### Reversible process at constant volume {#reversible-process-at-constant-volume .unnumbered}

Consider an infinitesimal reversible process at constant volume
($\delta w = 0$, $dV = 0$), in which the system exchanges only heat with
the surroundings:

$$dE = \delta q_\mathrm{rev}.$$

By definition of temperature, we can write:

$$dS = \frac{\delta q_\mathrm{rev}}{T}.$$

The change in entropy between two equilibrium states is obtained by
integration along a reversible path:

$$\Delta S = \int \frac{\delta q_\mathrm{rev}}{T}.$$

### Connection to Gibbs entropy {#connection-to-gibbs-entropy .unnumbered}

In the canonical ensemble, the system exchanges energy with a large
bath. For a quasi-static (infinitesimal) process, the **average energy
change** of the system corresponds to the heat exchanged:

$$d\langle E \rangle = \delta q_\mathrm{rev}.$$

Using the bath-based temperature $T$ defined previously, we then have:

$$dS_\mathrm{system} = \frac{d\langle E \rangle}{T} = \frac{\delta q_\mathrm{rev}}{T}.$$

Thus, the Gibbs/statistical definition of entropy naturally reduces to
the classical Carnot definition for reversible processes, and the two
are fully consistent.
