# Math Prerequisites

## Probability

Definition:\
[event]{.underline} := outcome\
[population]{.underline}: A set of similar items (events) that you are
interested in.\
[sample]{.underline}: a subset of population.\
In this class, we assume that population is well represented by a
sample. In other words, all charactoristics of population can be
recovered from a sample set.

### Discrete Random Variable

A discrete random variable is a function from a sample space $S$ to a
countable set $B$: $$X : S \rightarrow B.$$ For each outcome $s \in S$,
the random variable $X$ assigns a countable value (e.g., an integer).

**Example.** Consider a sample of 100 U.S. pennies. Suppose that 5 of
them were minted in 1970. Then the probability of selecting a penny made
in 1970 is $5\% = 5/100 = 0.05$. We write
$$\mathbb{P}(X = 1970) = 0.05.$$

For a discrete random variable $X$, we define the *probability mass
function* (PMF) by $$p(x) := \mathbb{P}(X = x).$$ Thus, in this example,
$$p(1970) = 0.05.$$

Probability is defined as a measure of the likelihood that an event
occurs. The probability mass function satisfies the following
properties: $$\sum_{x \in B} p(x) = 1, \qquad p(x) \ge 0.$$

Since the United States began minting pennies in 1776, we may write
$$\sum_{x=1776}^{\text{current year}} p(x) = 1,$$ which accounts for all
possible years in which a penny could have been minted.

$\star$ **Multiple Discrete Random Variables**

Let $X$ and $Y$ be discrete random variables with ranges $A$ and $B$,
respectively. The *joint probability mass function* (joint PMF) is
defined by $$p_{X,Y}(x,y) := \mathbb{P}(X = x, Y = y).$$

The joint PMF satisfies the following properties:
$$\sum_{x \in A} \sum_{y \in B} p_{X,Y}(x,y) = 1, \qquad p_{X,Y}(x,y) \ge 0.$$

The *marginal probability mass functions* are obtained by summing over
the other variable: $$p_X(x) = \sum_{y \in B} p_{X,Y}(x,y), \qquad
p_Y(y) = \sum_{x \in A} p_{X,Y}(x,y).$$

**Example.** Let $X$ denote the outcome of rolling a fair die, with
$$A = \{1,2,3,4,5,6\},$$ and let $Y$ denote the outcome of flipping a
fair coin, with $$B = \{h,t\}.$$ The joint PMF $p_{X,Y}(x,y)$ describes
the probability of observing each pair $(x,y)$.

$\star$ **Conditional Probability**

The *conditional probability* of observing event $A$ given that event
$B$ has occurred is defined by
$$\mathbb{P}(A \mid B) := \frac{\mathbb{P}(A,B)}{\mathbb{P}(B)}, 
\qquad \mathbb{P}(B) > 0.$$

From this definition, we obtain
$$\mathbb{P}(A,B) = \mathbb{P}(A \mid B)\,\mathbb{P}(B)
               = \mathbb{P}(B \mid A)\,\mathbb{P}(A).$$

**Bayes' Theorem.**
$$\mathbb{P}(A \mid B) = \frac{\mathbb{P}(B \mid A)\,\mathbb{P}(A)}{\mathbb{P}(B)}.$$

If two random variables $X$ and $Y$ are *independent*, then
$$\mathbb{P}(X=x, Y=y) = \mathbb{P}_X(x)\,\mathbb{P}_Y(y),$$ and
consequently, $$\mathbb{P}(X=x \mid Y=y) = \mathbb{P}_X(x).$$

$\star$ **Mean and Moments**

Let $X$ be a discrete random variable with probability mass function
$p(x)$. For any function $f(X)$, the *expected value* (or mean) is
defined as $$\langle f(X) \rangle := \sum_{x \in B} f(x)\,p(x).$$

In particular, the mean of $X$ is
$$\mu := \langle X \rangle = \sum_{x \in B} x\,p(x).$$

The *variance* (second central moment) of $X$ is defined by
$$\mathrm{Var}(X) := \langle (X-\mu)^2 \rangle
                 = \langle X^2 \rangle - \langle X \rangle^2
                 = \sum_{x \in B} x^2 p(x) - \left(\sum_{x \in B} x p(x)\right)^2.$$

The *standard deviation* is defined as
$$\sigma := \sqrt{\mathrm{Var}(X)}.$$

**Example.** The average dollar value of pennies in circulation may
exceed one cent because older pennies tend to have higher collectible
value. However, both the variance and the standard deviation are
relatively small, since very old pennies are rarely found in
circulation.

[**HW**]{.underline}\
Show that the variance is equal to the square of the standard deviation,
i.e., $$\mathrm{Var}(X) = \sigma^2.$$

$\star$ Correlation\
Covariance := Measure of the joint variability of two random variable.\
Basically, for random variable X and Y on joint support S, covariance
is\
$$\begin{array}{cccc} 
Cov(XY) = & <XY> &- & <X><Y>\\
                 & \uparrow & & \nwarrow \nearrow \\
                 & \begin{tabular}[t]{c} Mean value of  $XY$ \\ --- Joint prob  $\mathbb{P}(a,b)$\end{tabular} & & \text{Indep. mean value} 
\end{array}$$ More useful form is the normalized version of covarience
(correlation coefficient)\

::: center
Correlation Coefficient: $\rho_{XY} = \frac{Cov(XY)}{\sigma_X\sigma_Y}$
:::

This value is useful becaause\
$\rho_{XY} =1  \implies X\& Y$ are in perfect linear correlation\
$\rho_{XY} =0  \implies X\& Y$ are completely independent\
$\rho_{XY} =-1  \implies X\& Y$ are in perfect linear correlation
(negatively)\
Using the definitions up to this point, we can model a single die. i.e.
random variable 1,2,3,4,5, and 6 with probability of 1/6 for each
outcome. The mean is 3.5 and etc. We will consider system of dice (i.e.
20 dice)\
Define new randome variable $S = \sum\limits_{j=1}^N X_j$ i.e. total sum
of 20 dice\
Because sum is interchangeable,\
$$<S> = \sum\limits_j^N<X_j>$$\
Because dice are completely independent of each other,\
$$\sigma_S^2 = \sum\limits_j^N \sigma_j^2$$

[HW]{.underline} Show this is true. This is complecated so proof by
Excel is okay.\
Because dice is identical (If $N$ molecules in the system are
identical),\
$$<S> = N<X>$$ $$\sigma_S^2 = N \sigma_X^2$$
$$\sigma_S = \sqrt{N} \sigma_X$$

then, the coefficient of variation (standard deviation/mean) is

$$\frac{\sigma_S}{<S>} = \frac{1}{\sqrt{N}}\frac{\sigma_X}{<X>}$$

<figure data-latex-placement="h">
<div class="picture">
<p>(200,135) (0,0)<span><img src="NumbreofDie.png" style="width:8cm"
alt="image" /></span></p>
</div>
<figcaption>Coefficient of Variation</figcaption>
</figure>

$\star$ **Binomial Distribution**

Consider a random experiment with two possible outcomes: *success*
(True) and *failure* (False). Let $X$ be a discrete random variable for
a single trial:

$$X = 
\begin{cases}
1, & \text{with probability } p = \mathbb{P}(\text{success})\\
0, & \text{with probability } 1-p = \mathbb{P}(\text{failure})
\end{cases}$$

The mean (expected value) and variance of a single trial are:

$$\langle X \rangle = p, \qquad
\langle X^2 \rangle = 1^2 \cdot p + 0^2 \cdot (1-p) = p,$$

$$\sigma^2 = \mathrm{Var}(X) = \langle X^2 \rangle - \langle X \rangle^2 = p - p^2 = p(1-p).$$

For $N$ independent and identical trials, let $S = \sum_{i=1}^{N} X_i$
denote the total number of successes. Then

$$\langle S \rangle = N p, \qquad 
\mathrm{Var}(S) = N p (1-p) \quad \cdots \text{\textcircled{1}}.$$

The number of distinct sequences with exactly $n$ successes (True
outcomes) is given by

$$\binom{N}{n} = \frac{N!}{n!(N-n)!}.$$

Therefore, the probability of observing exactly $n$ successes in $N$
independent trials (the *binomial distribution*) is

$$\mathbb{P}(S=n) = \binom{N}{n} p^n (1-p)^{N-n}.$$

As $N \to \infty$, the binomial distribution approaches the normal
(Gaussian) distribution:

$$G(S) = \frac{1}{\sqrt{2 \pi \sigma^2}} 
\exp\left[-\frac{(S - \langle S \rangle)^2}{2 \sigma^2}\right] 
\quad \cdots \text{\textcircled{2}},$$

where $\langle S \rangle = N p$ and $\sigma^2 = N p(1-p)$.

This normal distribution arises frequently in physical chemistry and
many other applications.

[**HW**]{.underline}: Use 1⃝ and 2⃝ to plot the probability distribution
for $N = 1{,}000$ and $N = 10{,}000$.

###  Continuous Probability

Probability density is

continuous rand \# such that

$$\mathbb{P}([l,u]) = \int_l^u \rho(x)dx$$

$$\text{where } \int_{-\infty}^{\infty} \rho(x)dx = 1 \cdots \text{ normalized}$$

Similar to discrete case

$n$th noment is given by

$$<x^n> = \int_{-\infty}^{\infty} x^n \rho(x)dx \rightarrow \text{ What is average? }$$

& var is given by

$$<(x-<x>)^2> = \int_{-\infty}^{\infty}(x-<x>)^2\rho(x)dx$$

$\star$ multiple cont var

Joint prob $\rho(x,y)dxdy$

$$\rho_x(x) = \int_{-\infty}^{\infty}\rho(x,y)dy$$

$$\rho(y|x) = \frac{\rho(x,y)}{\rho_x(x)} \leftarrow \text{ conditional prob }$$

## Other Mathematical Concepts

$\star$ Dirac Delta Func $\delta$ has property\

<!-- rule removed for sphinx -->
     $\delta(x) = 0$ if $x \neq 0$
     $\int_{-\infty}^{\infty}\delta(x)dx = 1$
<!-- rule removed for sphinx -->

We use this to get a value of a func at $x_0$.\
$$f(x_0) = \int_{-\infty}^{\infty} f(x)\delta(x-x_0)dx$$

$\star$ Stirling's Formula\
$$\ln N! = \sum\limits_{n=1}^N \simeq N\ln N - N + 1 \simeq N\ln N - N$$

$\star$ Lagrange Multipliters (Constraint Search Method)\
The min or max of a func $f(x_1, x_2, \cdots, x_n)$

can be found by indentifying ${x_i}$ such that

$$\delta f = \sum\limits_{i=1}^n \left(\frac{\partial f}{\partial x_i}\right) \delta x_i = 0$$

For set of $r$ constraints, $g_j(x_1, x_2, \cdots, x_n) = 0$,
$j=1,2,\cdots,r$

min, max of $f(x_1, x_2, \cdots, x_n)$ can be found by finding $\{x_i\}$

that satisfy

$$\delta \mathscr{L} = \sum\limits_{i=1}^n \left[ \left(\frac{\partial f}{\partial x}\right) - \sum\limits_{j=1}^r \lambda_j \left(\frac{\partial g_j}{\partial x_i}\right) \right] \delta x_i = 0$$

Another way of wrtiting under single constraint

::: center
+:-------------------------------------------------------------------------------------------------------------------------------+:-------------------------------+
| $\left\{ \begin{array}{l} \bigtriangledown f = \lambda \bigtriangledown g \\ g(x_1, x_2, \cdots, x_n) = c \end{array} \right.$ |   ---------------------------- |
|                                                                                                                                |    $n + 1$ Eq $n + 1$ unknown  |
|                                                                                                                                |           $\Downarrow$         |
|                                                                                                                                |             solvable           |
|                                                                                                                                |   ---------------------------- |
+--------------------------------------------------------------------------------------------------------------------------------+--------------------------------+
:::

<figure data-latex-placement="h">
<div class="picture">
<p>(0,180) (-125,0)<span><img src="Lmult.png" style="height:6.5cm"
alt="image" /></span></p>
</div>
<p><br />
</p>
<figcaption>Maximum (minimum) points on a constraint.</figcaption>
</figure>

Simple example\

::: center
<hr/>
   Minimize  $f(x,y) = x^2+y^2$
      on     $g(x,y) => xy = 3$
<hr/>
:::

$$L = L(x,y,\lambda) = x^2 + y^2 - \lambda(xy-3)$$

$$Lx = 2x-\lambda y = 0 => \lambda = \frac{2x}{y}$$

$$Ly = 2y-\lambda x = 0 => \lambda = \frac{2y}{x}$$

$$\Downarrow$$

$$2x^2 = 2y^2 => x = \pm y$$

If $x = y$, then from $xy = 3$,\
$$x = \sqrt{3} \hspace{10pt} y = \sqrt{3} \hspace{10pt} \text{ or } (-\sqrt{3}, -\sqrt{3})$$

Before Legendre Transformation, we define convex function.\
An intuitive definition is that a function is convex if the line segment
connecting any two points on its graph always lies above or on the graph
itself.\

<figure data-latex-placement="h">
<div class="picture">
<p>(0,170) (-120,0)<span><img src="Convex.png" style="height:6cm"
alt="image" /></span></p>
</div>
<p><br />
</p>
<figcaption><span class="math inline">\(x^2\)</span> is a convex
function</figcaption>
</figure>

$\star$ Legendre Transformation\

The Legendre transformation is a mathematical procedure that
re-expresses a function in terms of a different independent variable,
usually one that is more natural for the physical problem. It replaces a
variable with its conjugate (such as replacing position with momentum,
or volume with pressure) by performing a change of variables that
preserves all the essential information of the original function. In
thermodynamics and classical mechanics, this transformation allows us to
switch from one potential to another---such as from internal energy to
enthalpy or from the Lagrangian to the Hamiltonian---by exchanging a
difficult variable for one that is easier to control or interpret.

Some functions have inconvenient natural variables. For example, the
internal energy $U(S,V)$ uses entropy as a variable, but experiments
typically control temperature instead. The Legendre transform provides a
systematic way to replace a variable $x$ with its conjugate
$\displaystyle p = \frac{df}{dx}$.

Let $f(x)$ be a differentiable convex function and define
$$p = \frac{df}{dx}.$$ The Legendre transform $g(p)$ of $f(x)$ is
defined as $$g(p) = px - f(x),$$ where $x$ is understood as a function
of $p$ obtained from the relation $p = df/dx$. The transformation
replaces dependence on $x$ by dependence on its conjugate variable $p$.

Geometrically, the Legendre transform represents $f(x)$ in terms of the
slopes of its tangent lines: $g(p)$ is the negative of the $y$-intercept
of the tangent line to $f$ with slope $p$.

**Math example:** This example shows that the collection of tangent
lines has an envelope (a curve that is tangent to all of them) that
exactly reconstructs the original function. Thus, converting from the
variable $x$ to its conjugate variable $p$ through the Legendre
transform loses no information.

<figure data-latex-placement="h">
<div class="picture">
<p>(0,170) (-120,0)<span><img src="Tangent.png" style="height:6cm"
alt="image" /></span></p>
</div>
<p><br />
</p>
<figcaption><span class="math inline">\(x^2\)</span> and tangent lines
</figcaption>
</figure>

$$p = \frac{df}{dx}=2x.$$

$$g(p) = px - f(x) = \frac{p^2}{2} - \frac{p^2}{4}=\frac{p^2}{4},$$

simply plug $p=2x$ back into this expression will get the original
function back.\

**Classical Mechanics:** Use the definition in next page to check he
transformation from the Lagrangian $L(q,\dot q)$ to the Hamiltonian:
$$p = \frac{\partial L}{\partial \dot q}, \qquad 
H(q,p) = p\dot q - L.$$ Can you guess the reason why why it is okay to
take a derivative w.r.t. $\dot q$ but we can't do the same for $q$?\
This is a good opportunity to look up what these things are in upper
devision physics textbook. For anyone interested in molecular dynamics
simulation, the software uses Hamiltonian to produce the trajectory of
the system
